[
  {
    "_id": "23939",
    "text": "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback: Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textualentailment rewards to optimize for factual consistency and explore the ensuing trade-offs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience and conciseness of the generated summaries.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "The Challenge of Factual Consistency in Abstractive Summarization",
      "Textual Entailment and Reinforcement Learning in NLP",
      "Reinforcement Learning with Textual Entailment Rewards",
      "Model Architecture and Training Procedure",
      "Improving Faithfulness, Salience, and Conciseness",
      "Balancing Consistency and Information Density"
    ],
    "keywords": [
      "Factually Consistent Summarization",
      "Reinforcement Learning",
      "Text Generation Systems",
      "Abstractive Summarization",
      "Textual Entailment",
      "Factual Inconsistency",
      "Reference-Free Rewards",
      "Faithfulness",
      "Salience",
      "Conciseness",
      "Automated Metrics",
      "Human Evaluation",
      "Textual Entailment Models",
      "Trade-offs in Summarization",
      "Informational Content",
      "Extractive vs Abstractive Summarization",
      "Grounded Text Generation",
      "Summarization Tasks",
      "Model Optimization",
      "Natural Language Processing",
      "Machine Learning Techniques",
      "Text Generation",
      "Factual Consistency",
      "Summarization",
      "NLP",
      "Reward Mechanisms",
      "Evaluation Methods",
      "Conceptual Themes"
    ]
  },
  {
    "_id": "28739",
    "text": "IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks: Backdoor attacks are an insidious security threat against machine learning models. Adversaries can manipulate the predictions of compromised models by inserting triggers into the training phase. Various backdoor attacks have been devised which can achieve nearly perfect attack success without affecting model predictions for clean inputs. Means of mitigating such vulnerabilities are underdeveloped, especially in natural language processing. To fill this gap, we introduce IMBERT, which uses either gradients or self-attention scores derived from victim models to self-defend against backdoor attacks at inference time. Our empirical studies demonstrate that IMBERT can effectively identify up to 98.5% of inserted triggers. Thus, it significantly reduces the attack success rate while attaining competitive accuracy on the clean dataset across widespread insertion-based attacks compared to two baselines. Finally, we show that our approach is model-agnostic, and can be easily ported to several pre-trained transformer models. * Now at Google DeepMind. 1 According to statistics from Hugging Face, BERT receives 15M downloads per month.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing (NLP) Security",
    "sections": [
      "Introduction to Backdoor Attacks in NLP",
      "Motivation and Research Gap",
      "Overview of IMBERT: A Novel Defense Mechanism",
      "Technical Framework of IMBERT",
      "Empirical Evaluation of IMBERT",
      "Model-Agnostic Design and Portability",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Backdoor Attacks",
      "BERT",
      "Insertion-based Attacks",
      "Machine Learning Security",
      "Adversarial Manipulation",
      "Trigger",
      "Model Compromise",
      "Inference Time Defense",
      "Self-Attention Scores",
      "Gradient Analysis",
      "Natural Language Processing",
      "Attack Success Rate",
      "Clean Inputs",
      "Model Agnosticism",
      "Pre-trained Transformer Models",
      "Hugging Face",
      "Security Vulnerabilities",
      "Empirical Studies",
      "Competitive Accuracy",
      "DeepMind"
    ]
  },
  {
    "_id": "18571",
    "text": "Extracting Topics from Texts Based on Situations: To understand text, we must relate it with specified situations. This paper, on the basis of such an idea, discusses how the things that a text describes and the situation that the text relates to are expressed in a computer and how the topic of a text is extracted.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "text analysis",
    "sections": [
      "The Role of Context in Text Understanding",
      "Situational Text Analysis Theoretical Foundations",
      "Challenges in Topic Extraction from Situational Texts",
      "Methodologies for Situational Topic Extraction",
      "Implementation and Case Studies in Situational Topic Extraction",
      "Evaluation of Situational Topic Models",
      "Future Directions in Situational Topic Extraction Research"
    ],
    "keywords": [
      "text analysis",
      "topic extraction",
      "situation based understanding",
      "textual semantics",
      "computational linguistics",
      "natural language processing",
      "contextual relevance",
      "semantic representation",
      "text classification",
      "information retrieval",
      "discourse analysis",
      "document thematic modeling",
      "machine learning for text",
      "textual contextualization",
      "situation awareness",
      "textual interpretation",
      "conceptual mapping",
      "ontology construction",
      "textual meaning generation",
      "computational text understanding"
    ]
  },
  {
    "_id": "35009",
    "text": "HyTER: Meaning-Equivalent Semantics for Translation Evaluation: It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation Evaluation",
    "sections": [
      "The Ambiguity of Translation as a Core Challenge",
      "Rationale for Empirical Estimation of Translation Ambiguity",
      "Design and Functionality of the Annotation Tool",
      "Quantifying Translation Ambiguity Through Empirical Data",
      "HyTER: A Novel Metric for Translation Accuracy",
      "Comparative Evaluation of HyTER Against Existing Metrics"
    ],
    "keywords": [
      "translation evaluation",
      "meaning equivalent semantics",
      "ambiguity in translation",
      "1-to-n mapping",
      "hyter metric",
      "machine translation accuracy",
      "human translation accuracy",
      "annotation tool",
      "exponential number of translations",
      "natural language processing",
      "translation ambiguity estimation",
      "semantic representation",
      "translation metrics",
      "empirical estimates",
      "language models",
      "translation quality assessment",
      "semantic equivalence",
      "translation ambiguity",
      "linguistic analysis",
      "computational linguistics"
    ]
  },
  {
    "_id": "24670",
    "text": "AGIF: An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling: In real-world scenarios, users usually have multiple intents in the same utterance. Unfortunately, most spoken language understanding (SLU) models either mainly focused on the single intent scenario, or simply incorporated an overall intent context vector for all tokens, ignoring the fine-grained multiple intents information integration for token-level slot prediction. In this paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint multiple intent detection and slot filling, where we introduce an intent-slot graph interaction layer to model the strong correlation between the slot and intents. Such an interaction layer is applied to each token adaptively, which has the advantage to automatically extract the relevant intents information, making a fine-grained intent information integration for the token-level slot prediction. Experimental results on three multiintent datasets show that our framework obtains substantial improvement and achieves the state-of-the-art performance. In addition, our framework achieves new state-of-the-art performance on two single-intent datasets.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Challenges in Multi-Intent Spoken Language Understanding",
      "Survey of Existing Approaches to Intent Detection and Slot Filling",
      "Introducing the Adaptive Graph-Interactive Framework (AGIF)",
      "Design and Components of AGIF",
      "Performance Assessment on Multi-Intent Datasets",
      "State-of-the-Art Performance on Single-Intent Tasks"
    ],
    "keywords": [
      "Adaptive Graph-Interactive Framework",
      "Multiple Intent Detection",
      "Slot Filling",
      "Spoken Language Understanding",
      "Joint Task Learning",
      "Intent-Slot Graph Interaction",
      "Token-Level Slot Prediction",
      "Fine-Grained Intent Integration",
      "Single Intent Scenario",
      "Multi-Intent Datasets",
      "State-of-the-Art Performance",
      "Graph-Based Modeling",
      "Natural Language Processing",
      "Dialogue Systems",
      "Context Vector Integration",
      "Adaptive Interaction Mechanism",
      "Intent Correlation Modeling",
      "Utterance Analysis",
      "Model Adaptability",
      "Speech Recognition",
      "Understanding",
      "NLP",
      "Dialog Systems",
      "SLU",
      "Graph Modeling",
      "Intent Detection",
      "Slot Prediction",
      "Joint Learning",
      "Context Integration",
      "Adaptive Mechanism"
    ]
  },
  {
    "_id": "4778",
    "text": "Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector: We investigate the recently developed Bidirectional Encoder Representations from Transformers (BERT) model(Devlin et al., 2018)for the hyperpartisan news detection task. Using a subset of hand-labeled articles from Se-mEval as a validation set, we test the performance of different parameters for BERT models. We find that accuracy from two different BERT models using different proportions of the articles is consistently high, with our bestperforming model on the validation set achieving 85% accuracy and the best-performing model on the test set achieving 77%. We further determined that our model exhibits strong consistency, labeling independent slices of the same article identically. Finally, we find that randomizing the order of word pieces dramatically reduces validation accuracy (to approximately 60%), but that shuffling groups of four or more word pieces maintains an accuracy of about 80%, indicating the model mainly gains value from local context.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Text Classification and Sentiment Analysis",
    "sections": [
      "Introduction to the Task and Model",
      "Methodology and Experimental Setup",
      "Model Performance Evaluation",
      "Consistency of Model Predictions",
      "Impact of Word Order on Model Performance",
      "Conclusion and Implications"
    ],
    "keywords": [
      "Hyperpartisan News Detection",
      "BERT",
      "SemEval-2019",
      "Accuracy",
      "Validation Set",
      "Test Set",
      "Model Parameters",
      "Word Pieces",
      "Contextual Understanding",
      "Local Context",
      "Consistency",
      "Randomization",
      "Shuffling",
      "Tokenization",
      "NLP",
      "Machine Learning",
      "Text Classification",
      "Sentiment Analysis",
      "Information Retrieval",
      "Social Media Monitoring"
    ]
  },
  {
    "_id": "14662",
    "text": "Automatically Creating Bilingual Lexicons for Machine Translation from Bilingual Text: A method is presented for automatically augmenting the bilingual lexicon of an existing Machine Translation system, by extracting bilingual entries from aligned bilingual text. The proposed method only relies on the resources already available in the MT system itself. It is based on the use of bilingual lexical templates to match the terminal symbols in the parses of the aligned sentences.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation and NLP Techniques",
    "sections": [
      "Introduction to Bilingual Lexicons in Machine Translation",
      "Overview of Existing Methods for Lexicon Augmentation",
      "Proposed Method: Automated Extraction Using Bilingual Lexical Templates",
      "Implementation Details and System Architecture",
      "Evaluation and Results",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Machine Translation",
      "Bilingual Lexicon",
      "Lexicon Augmentation",
      "Bilingual Text",
      "Aligned Sentences",
      "Parsing",
      "Terminal Symbols",
      "Bilingual Lexical Templates",
      "Automated Methods",
      "Natural Language Processing",
      "Translation Systems",
      "Language Resources",
      "Syntactic Analysis",
      "Cross-Linguistic Alignment",
      "Lexical Extraction",
      "Translation Models",
      "Semantic Mapping",
      "Resource-Driven Approaches",
      "Text Alignment",
      "Language Pairing"
    ]
  },
  {
    "_id": "36618",
    "text": "Attribute-Based and Value-Based Clustering: An Evaluation: In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color).We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Concept Representation and Lexical Clustering",
    "sections": [
      "Introduction to Concept Acquisition from Corpora",
      "Role of Attributes and Values in Concept Representation",
      "Methodology for Comparing Clustering Models",
      "Performance Evaluation of Attribute-Based Models",
      "Superiority of Mixed Attribute-Value Models",
      "Implications for Concept Modeling and NLP"
    ],
    "keywords": [
      "Concept Acquisition",
      "Corpora",
      "Attribute-Based Clustering",
      "Value-Based Clustering",
      "Lexical Clustering",
      "Concept Modeling",
      "Syntactic Structures",
      "Modifiers",
      "Attribute Values",
      "Knowledge Representation",
      "Conceptual Definitions",
      "Color Attributes",
      "Natural Language Processing",
      "Semantic Analysis",
      "Model Evaluation",
      "Descriptive Efficiency",
      "Mixed Models",
      "Conceptual Ontology",
      "Lexical Semantics",
      "Information Retrieval"
    ]
  },
  {
    "_id": "15957",
    "text": "Improving Entity Linking using Surface Form Refinement: In this paper, we present an algorithm for improving named entity resolution and entity linking by using surface form generation and rewriting. Surface forms consist of a word or a group of words that matches lexical units like Paris or New York City. Used as matching sequences to select candidate entries in a knowledge base, they contribute to the disambiguation of those candidates through similarity measures. In this context, misspelled textual sequences (entities) can be impossible to identify due to the lack of available matching surface forms. To address this problem, we propose an algorithm for surface form refinement based on Wikipedia resources. The approach extends the surface form coverage of our entity linking system, and rewrites or reformulates misspelled mentions (entities) prior to starting the annotation process. The algorithm is evaluated on the corpus associated with the monolingual English entity linking task of NIST KBP 2013. We show that the algorithm improves the entity linking system performance.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "Entity Linking and Resolution",
    "sections": [
      "Introduction to Entity Linking and Surface Forms",
      "Challenges in Entity Linking Due to Misspelled Surface Forms",
      "Proposed Algorithm: Surface Form Refinement",
      "How Surface Form Refinement Works in Practice",
      "Experimental Setup and Performance Analysis",
      "Implications and Directions for Further Research"
    ],
    "keywords": [
      "Entity Linking",
      "Named Entity Resolution",
      "Surface Form Generation",
      "Surface Form Rewriting",
      "Lexical Units",
      "Knowledge Base Matching",
      "Disambiguation",
      "Similarity Measures",
      "Misspelled Entities",
      "Surface Form Coverage",
      "Algorithm Development",
      "Wikipedia Resources",
      "Entity Annotation Process",
      "NIST KBP 2013 Corpus",
      "Monolingual Entity Linking",
      "Textual Discrepancy Handling",
      "Natural Language Processing",
      "Information Retrieval",
      "Semantic Disambiguation",
      "Machine Learning for NER"
    ]
  },
  {
    "_id": "11159",
    "text": "A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora: Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a bestfirst strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted.For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation",
    "sections": [
      "Challenges in Automatic Transfer Mapping Extraction",
      "Best-First Strategy with Alignment Grammar",
      "Mechanisms for Frequency Computation and Context Retention",
      "Assessment of Translation Quality",
      "Corpus Details and Baseline Comparisons",
      "Performance Evaluation and Quality Improvements",
      "Impacts on Transfer Learning and NLP Applications",
      "Summary and Future Work"
    ],
    "keywords": [
      "best first alignment",
      "transfer mappings",
      "bilingual corpora",
      "automatic extraction",
      "translation systems",
      "alignment accuracy",
      "mapping quality",
      "small alignment grammar",
      "context retention",
      "competing mappings",
      "frequency computation",
      "translation quality evaluation",
      "corpus size",
      "algorithm variants",
      "natural language processing",
      "machine translation",
      "rule based systems",
      "example based learning",
      "language modeling",
      "computational linguistics"
    ]
  }
]