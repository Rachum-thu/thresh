[
  {
    "_id": "35194",
    "text": "A Re-examination of Query Expansion Using Lexical Resources: Query expansion is an effective technique to improve the performance of information retrieval systems. Although hand-crafted lexical resources, such as WordNet, could provide more reliable related terms, previous studies showed that query expansion using only WordNet leads to very limited performance improvement. One of the main challenges is how to assign appropriate weights to expanded terms. In this paper, we re-examine this problem using recently proposed axiomatic approaches and find that, with appropriate term weighting strategy, we are able to exploit the information from lexical resources to significantly improve the retrieval performance.Our empirical results on six TREC collections show that query expansion using only hand-crafted lexical resources leads to significant performance improvement. The performance can be further improved if the proposed method is combined with query expansion using co-occurrence-based resources.",
    "coarse_grained_category": "Computer Science",
    "fine_grained_category": "Information Retrieval",
    "sections": [
      "Introduction to Query Expansion and Its Role in Information Retrieval",
      "Challenges in Using Hand-Crafted Lexical Resources",
      "Axiomatic Approaches for Term Weighting",
      "Proposed Methodology: Integrating Lexical Resources with Axiomatic Weighting",
      "Empirical Evaluation on TREC Collections",
      "Comparative Analysis and Combined Approaches"
    ],
    "keywords": [
      "query expansion",
      "information retrieval systems",
      "lexical resources",
      "wordnet",
      "term weighting strategy",
      "axiomatic approaches",
      "performance improvement",
      "trec collections",
      "co-occurrence based resources",
      "hand crafted lexical resources",
      "retrieval performance",
      "related terms",
      "information retrieval evaluation",
      "lexical semantics",
      "query optimization",
      "natural language processing",
      "text mining",
      "relevance feedback",
      "semantic similarity",
      "resource integration in ir"
    ]
  },
  {
    "_id": "32162",
    "text": "Efficient Multilingual Text Classification for Indian Languages: India is one of the richest language hubs on the earth and is very diverse and multilingual. But apart from a few Indian languages, most of them are still considered to be resource poor. Since most of the NLP techniques either require linguistic knowledge that can only be developed by experts and native speakers of that language or they require a lot of labelled data which is again expensive to generate, the task of text classification becomes challenging for most of the Indian languages. The main objective of this paper is to see how one can benefit from the lexical similarity found in Indian languages in a multilingual scenario. Can a classification model trained on one Indian language be reused for other Indian languages? So, we performed zero-shot text classification via exploiting lexical similarity and we observed that our model performs best in those cases where the vocabulary overlap between the language datasets is maximum. Our experiments also confirm that a single multilingual model trained via exploiting language relatedness outperforms the baselines by significant margins.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction: The Multilingual Challenge in Indian Languages",
      "Challenges in Text Classification for Resource-Poor Languages",
      "Leveraging Lexical Similarity in Multilingual Settings",
      "Methodology: Zero-Shot Text Classification via Lexical Similarity",
      "Experimental Setup and Results",
      "Discussion and Implications"
    ],
    "keywords": [
      "Multilingual Text Classification",
      "Indian Languages",
      "Lexical Similarity",
      "Resource-Poor Languages",
      "Natural Language Processing",
      "Zero-Shot Learning",
      "Text Classification",
      "Language Diversity",
      "Linguistic Knowledge",
      "Labelled Data",
      "Model Reusability",
      "Vocabulary Overlap",
      "Multilingual Models",
      "Language Relatedness",
      "Expert Knowledge",
      "Native Speakers",
      "Data Generation Costs",
      "Baseline Models",
      "Performance Evaluation",
      "Cross-Lingual Transfer"
    ]
  },
  {
    "_id": "18220",
    "text": "Harvesting and Refining Question-Answer Pairs for Unsupervised QA: Question Answering (QA) has shown great success thanks to the availability of largescale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as REFQA). Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RE-FQA. We conduct experiments 1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Question Answering (QA) Systems",
    "sections": [
      "Introduction to Unsupervised Question Answering",
      "Harvesting Diverse Question-Answer Pairs",
      "Refinement of Question-Answer Pairs Using QA Models",
      "Experimental Setup and Evaluation Metrics",
      "Results and Comparative Analysis",
      "Few-Shot Learning Evaluation and Broader Implications"
    ],
    "keywords": [
      "Question Answering",
      "Unsupervised QA",
      "Large-scale Datasets",
      "Neural Models",
      "Few-shot Learning",
      "No-labeled Data",
      "Lexical Diversity",
      "Syntactic Diversity",
      "Wikipedia",
      "REFQA Corpus",
      "Question-Answer Pair",
      "Iterative Refinement",
      "BERT Model",
      "Fine-tuning",
      "SQuAD 1.1",
      "NewsQA",
      "Supervised Learning",
      "Unsupervised Learning",
      "Data Augmentation",
      "Automated Corpus",
      "Model Evaluation"
    ]
  },
  {
    "_id": "24519",
    "text": "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages: Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.",
    "coarse_grained_category": "Human-Computer Interaction (HCI)",
    "fine_grained_category": "Natural Language Processing (NLP) and Text Prediction",
    "sections": [
      "The Role of Text Prediction in Modern Communication",
      "Understanding Text Prediction Algorithms and Their Limitations",
      "The Importance of Contextual Signals in Text Prediction",
      "Contextual Signals in Chat Communication: A Case Study on Microsoft Teams",
      "Contextual Signals in Email Communication: Insights from Outlook",
      "Comparative Analysis: Chat vs. Email Text Prediction with Context"
    ],
    "keywords": [
      "Text Prediction",
      "Contextual Signals",
      "Chat Communication",
      "Email Communication",
      "Productivity",
      "Real-Time Phrase Completion",
      "Large Language Models",
      "Prior Words",
      "Commercial Text Prediction Model",
      "Microsoft Teams",
      "Outlook",
      "Time Context",
      "Previous Messages",
      "Service-Oriented Text Prediction",
      "Performance Metrics",
      "Contextual Awareness",
      "User Interaction",
      "Task Completion",
      "Natural Language Processing",
      "Contextual Adaptation"
    ]
  },
  {
    "_id": "54352",
    "text": "A Simple Method for Tagset Comparison: Based on the idea that local contexts predict the same basic category across a language, we develop a simple method for comparing tagsets across corpora. The principle differences between tagsets are evidenced by variation in categories in one corpus in the same contexts where another corpus exhibits only a single tag. Such mismatches highlight differences in the definitions of tags which are crucial when porting technology from one annotation scheme to another.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP) and Annotation Schemes",
    "sections": [
      "Introduction to Tagset Comparison",
      "Local Contexts Predict Categories",
      "Identifying Tagset Mismatches",
      "Methodology for Tagset Comparison",
      "Applications and Implications",
      "Conclusion and Future Directions"
    ],
    "keywords": [
      "tagset comparison",
      "corpus analysis",
      "annotation schemes",
      "local contexts",
      "tag definitions",
      "language variation",
      "tag mismatches",
      "category variation",
      "cross-corpus analysis",
      "tag portability",
      "linguistic categories",
      "annotation transfer",
      "tagset alignment",
      "corpus-specific tags",
      "semantic equivalence",
      "tagset mapping",
      "lexical contexts",
      "tagset design",
      "computational linguistics",
      "natural language processing"
    ]
  },
  {
    "_id": "50359",
    "text": "GLTR: Statistical Detection and Visualization of Generated Text: The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing and Text Generation",
    "sections": [
      "The Rise of Text Generation and Its Implications",
      "The Need for Simple and Explainable Detection Methods",
      "Overview of GLTR: A Tool for Detecting Generated Text",
      "Methodology: Statistical Techniques for Detecting Generation Artifacts",
      "Evaluation: Human-Subjects Study Results",
      "Implementation and Deployment: Making GLTR Accessible",
      "Applications and Impact: Real-World Use Cases",
      "Conclusion: Advancing Transparency and Trust in AI Systems"
    ],
    "keywords": [
      "Generated Text",
      "Language Models",
      "Text Generation Systems",
      "Abuse of AI",
      "Statistical Detection",
      "GLTR",
      "Human Detection",
      "Fake Text Identification",
      "Open-Source Software",
      "Public Deployment",
      "Sampling Schemes",
      "Annotation Scheme",
      "Human-Subjects Study",
      "Non-Expert Users",
      "Model Outputs",
      "Detection Rate Improvement",
      "Artifacts in Text",
      "AI Accountability",
      "Ethical AI Use",
      "AI Transparency"
    ]
  },
  {
    "_id": "171",
    "text": "Think Visually: Question Answering through Virtual Imagery: In this paper, we study the problem of geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture designed for answering questions that admit latent visual representations. DSMN learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks 1 .",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Computer Vision and Natural Language Processing",
    "sections": [
      "Introduction to Geometric Reasoning in Question Answering",
      "Overview of Dynamic Spatial Memory Network (DSMN)",
      "Design and Functionality of DSMN",
      "Synthetic Benchmarks for Geometric Reasoning: FloorPlanQA and ShapeIntersection",
      "Experimental Setup and Evaluation Metrics",
      "Results and Analysis of DSMN Performance",
      "Discussion of Implications and Future Directions"
    ],
    "keywords": [
      "Geometric Reasoning",
      "Question Answering",
      "Virtual Imagery",
      "Dynamic Spatial Memory Network",
      "Visual Representation",
      "Deep Learning Architecture",
      "Synthetic Benchmarks",
      "FloorPlanQA",
      "ShapeIntersection",
      "Visual Thinking Tasks",
      "Latent Visual Representations",
      "Reasoning Over Visual Data",
      "Spatial Reasoning",
      "Model Evaluation",
      "Experimental Validation",
      "Artificial Intelligence",
      "Computer Vision",
      "Cognitive Modeling",
      "Deep Neural Networks",
      "Visual Question Answering"
    ]
  },
  {
    "_id": "31226",
    "text": "Annotating the Little Prince with Chinese AMRs: Meaning Representation (AMR) is an annotation framework in which the meaning of a full sentence is represented as a rooted, acyclic, directed graph. In this paper, we describe a pilot project in which we develop specifications for the annotation of a Chinese AMR corpus: the Chinese translation of the Little Prince. The interagreement smatch score between the two annotators is 0.83. We also propose to integrate alignment into Chinese AMR annotation.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Semantic Annotation and Representation",
    "sections": [
      "Introduction to Meaning Representation (AMR)",
      "Project Overview: Annotating the Chinese Translation of *The Little Prince*",
      "Methodology for Chinese AMR Annotation",
      "Interannotator Agreement and Evaluation",
      "Integration of Alignment in Chinese AMR Annotation",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Meaning Representation",
      "Annotation Framework",
      "Chinese AMR Corpus",
      "Little Prince",
      "Inter-agreement",
      "SMATCH Score",
      "Annotation Specifications",
      "Natural Language Processing",
      "Semantic Annotation",
      "Graph-based Representation",
      "Rooted Directed Graph",
      "Acyclic Graph",
      "Chinese Translation",
      "Corpus Development",
      "Annotation Pilot Project",
      "Semantic Alignment",
      "Cross-lingual Annotation",
      "Linguistic Annotation",
      "Semantic Parsing",
      "Computational Semantics"
    ]
  },
  {
    "_id": "47162",
    "text": "LIMSI Submission for the WMT'13 Quality Estimation Task: an Experiment with n-gram Posteriors: This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task. Our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation",
    "sections": [
      "Introduction and Motivation",
      "Related Work",
      "Methodology and Features",
      "Data and Preprocessing",
      "Experimental Setup",
      "Results and Discussion"
    ],
    "keywords": [
      "Quality Estimation",
      "Machine Learning",
      "n-gram Posteriors",
      "Shared Task",
      "System Output",
      "Probability Quantification",
      "Language Modeling",
      "Feature Engineering",
      "Machine Translation",
      "Statistical NLP",
      "Language Processing",
      "Evaluation Metrics",
      "Model Performance",
      "Text Generation",
      "Natural Language Understanding",
      "Predictive Modeling",
      "Language Resources",
      "Task-Specific Features",
      "Probabilistic Models",
      "Translation Quality Assessment"
    ]
  },
  {
    "_id": "27656",
    "text": "GRaSP: Grounded Representation and Source Perspective: When people or organizations provide information, they make choices regarding what they include and how they represent it. These two aspects combined (the content and the stance) represent a perspective. Investigating perspectives can provide useful insights into the reliability of information, changes in viewpoints over time, shared beliefs among social or political groups and contrasts with other groups, etc. This paper introduces GRaSP, a generic framework for modeling perspectives and their sources.",
    "coarse_grained_category": "Artificial Intelligence and Computational Linguistics",
    "fine_grained_category": "Perspective Analysis and Information Retrieval",
    "sections": [
      "Introduction to GRaSP: Purpose and Motivation",
      "Understanding Perspectives: Content and Stance",
      "Theoretical Foundations of GRaSP",
      "Modeling Perspectives and Their Sources",
      "Applications and Use Cases of GRaSP",
      "Implications and Future Directions"
    ],
    "keywords": [
      "GRaSP",
      "Grounded Representation",
      "Source Perspective",
      "Information Reliability",
      "Viewpoint Evolution",
      "Shared Beliefs",
      "Group Contrast",
      "Perspective Modeling",
      "Information Sources",
      "Content Selection",
      "Stance Representation",
      "Social Groups",
      "Political Groups",
      "Information Framing",
      "Cognitive Bias",
      "Narrative Construction",
      "Epistemic Trust",
      "Discourse Analysis",
      "Semantic Representation",
      "Contextual Awareness"
    ]
  }
]