[
  {
    "_id": "27086",
    "text": "Multilingual summarization system based on analyzing the discourse structure at MultiLing 2013: This paper describes the architecture of UAIC 1 's Summarization system participating at MultiLing -2013. The architecture includes language independent text processing modules, but also modules that are adapted for one language or another. In our experiments, the languages under consideration are Bulgarian, German, Greek, English, and Romanian. Our method exploits the cohesion and coherence properties of texts to build discourse structures. The output of the parsing process is used to extract general summaries.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "multilingual summarization systems",
    "sections": [
      "Background on MultiLing 2013 Shared Task and Multilingual Summarization",
      "UAIC 1 System Architecture and Modular Design",
      "Language-Specific Adaptation for Target Languages",
      "Discourse Structure Analysis for Coherence Modeling",
      "Text Parsing and Summary Extraction Workflow",
      "Experimental Evaluation Across Five Languages"
    ],
    "keywords": [
      "Multilingual Summarization",
      "Discourse Structure Analysis",
      "MultiLing 2013",
      "UAIC 1 Summarization System",
      "Language Independent Text Processing",
      "Language-Specific Modules",
      "Cohesion in Text",
      "Coherence in Text",
      "Discourse Parsing",
      "Text Summarization Output",
      "Cross-Linguistic Processing",
      "Bulgarian Language",
      "German Language",
      "Greek Language",
      "English Language",
      "Romanian Language",
      "Natural Language Processing",
      "Text Analysis Techniques",
      "Information Extraction",
      "Multilingual NLP Systems"
    ]
  },
  {
    "_id": "28737",
    "text": "Efficient Online Scalar Annotation with Bounded Support: We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments. We contrast direct assessment (annotators assign scores to items directly), online pairwise ranking aggregation (scores derive from annotator comparison of items), and a hybrid approach (EASL: Efficient Annotation of Scalar Labels) proposed here. Our proposal leads to increased correlation with ground truth, at far greater annotator efficiency, suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Human-Computer Interaction and Crowd Sourcing",
    "sections": [
      "Introduction to Scalar Annotation Challenges",
      "Overview of Annotation Methods",
      "Direct Assessment: Assigning Scores Directly",
      "Online Pairwise Ranking Aggregation",
      "EASL: Efficient Annotation of Scalar Labels",
      "Evaluation and Results",
      "Implications for Dataset Construction and System Evaluation"
    ],
    "keywords": [
      "Scalar Annotation",
      "Efficient Annotation",
      "Human Judgment",
      "Dataset Construction",
      "System Quality Estimation",
      "Direct Assessment",
      "Online Pairwise Ranking",
      "Aggregation",
      "Hybrid Approach",
      "EASL",
      "Ground Truth Correlation",
      "Annotator Efficiency",
      "Annotation Strategy",
      "Manual System Evaluation",
      "Crowdsourcing",
      "Labeling Methodology",
      "Comparative Judgment",
      "Data Annotation Techniques",
      "Quality Assessment",
      "Annotation Framework"
    ]
  },
  {
    "_id": "19347",
    "text": "Boosting Open Information Extraction with Noun-Based Relations: Open Information Extraction (Open IE) is a strategy for learning relations from texts, regardless the domain and without predefining these relations. Work in this area has focused mainly on verbal relations. In order to extend Open IE to extract relationships that are not expressed by verbs, we present a novel Open IE approach that extracts relations expressed in noun compounds (NCs), such as (oil, extracted from, olive) from \"olive oil\", or in adjective-noun pairs (ANs), such as (moon, that is, gorgeous) from \"gorgeous moon\". The approach consists of three steps: detection of NCs and ANs, interpretation of these compounds in view of corpus enrichment and extraction of relations from the enriched corpus. To confirm the feasibility of this method we created a prototype and evaluated the impact of the application of our proposal in two state-of-the-art Open IE extractors. Based on these tests we conclude that the proposed approach is an important step to fulfil the gap concerning the extraction of relations within the noun compounds and adjective-noun pairs in Open IE.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction to Open Information Extraction (Open IE)",
      "The Need for Extending Open IE Beyond Verbal Relations",
      "Overview of Noun Compounds (NCs) and Adjective-Noun Pairs (ANs)",
      "Proposed Approach: Extracting Relations from NCs and ANs",
      "Implementation and Prototype Development",
      "Evaluation of the Proposed Method",
      "Results and Analysis of the Evaluation",
      "Conclusion and Future Directions"
    ],
    "keywords": [
      "open information extraction",
      "noun based relations",
      "verbal relations",
      "noun compounds",
      "adjective noun pairs",
      "relation extraction",
      "corpus enrichment",
      "natural language processing",
      "text mining",
      "information extraction",
      "domain independent extraction",
      "unsupervised learning",
      "prototype development",
      "evaluation metrics",
      "state of the art systems",
      "semantic relations",
      "linguistic patterns",
      "syntactic structures",
      "knowledge representation",
      "information retrieval"
    ]
  },
  {
    "_id": "25306",
    "text": "Neural Topic Model with Reinforcement Learning: In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models. *",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Natural Language Processing",
    "sections": [
      "Introduction to Neural Topic Models",
      "Challenges with Traditional Reconstruction-Based Objectives",
      "Incorporating Reinforcement Learning for Topic Coherence",
      "Dynamic Separation of Background and Topic Words",
      "Experimental Setup and Datasets",
      "Results and Comparative Analysis"
    ],
    "keywords": [
      "Neural Topic Model",
      "Reinforcement Learning",
      "Variational Autoencoder",
      "Neural Variational Inference",
      "Text Processing",
      "Topic Coherence",
      "Reward Signals",
      "Latent Topic Vectors",
      "Document Reconstruction",
      "Reconstruction Error",
      "Topic Quality",
      "Background Words",
      "Topic Words",
      "Dynamic Separation",
      "Pre-Processing",
      "Infrequent Words",
      "Top Frequent Words",
      "Perplexity",
      "NIPS Dataset",
      "20 Newsgroups Dataset"
    ]
  },
  {
    "_id": "23573",
    "text": "A Repository of Corpora for Summarization: Summarization corpora are numerous but fragmented, making it challenging for researchers to efficiently pinpoint corpora most suited to a given summarization task. In this paper, we introduce a repository containing corpora available to train and evaluate automatic summarization systems. We also present an overview of the main corpora with respect to the different summarization tasks, and identify various corpus parameters that researchers may want to consider when choosing a corpus. Lastly, as the recent successes of artificial neural networks for summarization have renewed the interest in creating large-scale corpora for summarization, we survey which corpora are used in neural network research studies. We come to the conclusion that more large-scale corpora for summarization are needed. Furthermore, each corpus is organized differently, which makes it time-consuming for researchers to experiment a new summarization algorithm on many corpora, and as a result studies typically use one or very few corpora. Agreeing on a data standard for summarization corpora would be beneficial to the field.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction: The Fragmentation of Summarization Corpora",
      "Overview of the Proposed Repository",
      "Classification of Summarization Corpora by Task Type",
      "Key Corpus Parameters for Selection",
      "Survey of Corpora Used in Neural Network Research",
      "Conclusion and Recommendations for Future Work"
    ],
    "keywords": [
      "summarization corpora",
      "automatic summarization systems",
      "corpus repository",
      "neural networks",
      "large-scale corpora",
      "data standardization",
      "research challenges",
      "task-specific corpora",
      "corpus parameters",
      "evaluation metrics",
      "training data",
      "algorithm experimentation",
      "summarization tasks",
      "corpus organization",
      "research studies",
      "artificial intelligence",
      "natural language processing",
      "information retrieval",
      "data fragmentation",
      "standardization efforts"
    ]
  },
  {
    "_id": "32080",
    "text": "Which Performs Better on In-Vocabulary Word Segmentation: Based on Word or Character?: The work is done when the first author is working in MSRA as an intern. closed tests. Furthermore, our analysis shows that using confidence measure to combine the two segmentation results should be under certain limitation.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Context, Motivation, and Research Objectives",
      "Previous Studies on Word and Character-Based Segmentation",
      "Experimental Setup and Model Design",
      "Performance Comparison of Word and Character Models",
      "Insights into Model Behavior and Limitations",
      "Combining Segmentation Results Using Confidence Measures",
      "Summary of Findings and Directions for Further Research"
    ],
    "keywords": [
      "in-vocabulary segmentation",
      "word vs character segmentation",
      "performance comparison",
      "confidence measure",
      "segmentation results combination",
      "msra internship",
      "closed tests",
      "natural language processing",
      "text segmentation",
      "language modeling",
      "statistical nlp",
      "segmentation accuracy",
      "model evaluation",
      "machine learning models",
      "tokenization methods",
      "language specific processing",
      "segmentation limitations",
      "hybrid segmentation approaches",
      "model integration",
      "performance optimization"
    ]
  },
  {
    "_id": "34786",
    "text": "Post-editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain: Neural machine translation (NMT) has set new quality standards in automatic translation, yet its effect on post-editing productivity is still pending thorough investigation. We empirically test how the inclusion of NMT, in addition to domain-specific translation memories and termbases, impacts speed and quality in professional translation of financial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in-domain data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation and Post-Editing",
    "sections": [
      "Role of Neural Machine Translation in Modern Translation",
      "Research Context and Motivation for NMT Evaluation",
      "Methodology for Empirical Study Design",
      "Quantitative Results of Time and Quality Analysis",
      "Implications of NMT for Post-Editing Workflows",
      "Conclusion and Future Research Directions in Financial Translation"
    ],
    "keywords": [
      "Neural Machine Translation",
      "Post-editing Productivity",
      "Translation Quality",
      "Financial Texts",
      "Banking and Finance Domain",
      "Translation Memories",
      "Termbases",
      "Language Pairs",
      "System Adaptation",
      "In-domain Data",
      "Empirical Assessment",
      "Time Savings",
      "Professional Translation",
      "Automated Translation",
      "Human Post-editing",
      "Translation Efficiency",
      "Domain-Specific Translation",
      "Machine Translation Post-editing",
      "Quality Evaluation",
      "Translation Technology"
    ]
  },
  {
    "_id": "54789",
    "text": "A Systematic Investigation of Commonsense Knowledge in Large Language Models: Language models (LMs) trained on large amounts of data (e.g., Brown et al., 2020;Patwary et al., 2021)have shown impressive performance on many NLP tasks under the zeroshot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pretrained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Understanding the Role of Commonsense Knowledge in NLP Tasks",
      "The Rise of Large Language Models and Their Limitations",
      "Designing a Rigorous Evaluation Framework for Commonsense Knowledge",
      "Minimizing Confounding Factors in Commonsense Assessment",
      "Performance Across Different Model Sizes and Evaluation Settings",
      "Implications for Pretrained Language Models and Commonsense Learning",
      "Benchmarking Against Human-Level Commonsense Reasoning",
      "Toward More Robust and Human-like Commonsense Reasoning in LMs"
    ],
    "keywords": [
      "Commonsense Knowledge",
      "Large Language Models",
      "Zero-shot Learning",
      "Few-shot Learning",
      "Natural Language Processing",
      "Pretrained Language Models",
      "Task-specific Supervision",
      "Surface Cues",
      "Annotation Artefacts",
      "Evaluation Metrics",
      "Model Performance",
      "Human-Level Performance",
      "Systematic Evaluation",
      "Rigorous Methodology",
      "Model Limitations",
      "Data Training",
      "Model Size",
      "Language Understanding",
      "Conceptual Reasoning",
      "Cognitive Abilities"
    ]
  },
  {
    "_id": "2443",
    "text": "Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS: By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs.On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.These results pose important implications that are worth careful consideration for the further development and deployment of tool-and retrieval-augmented LLMs. 1 * The first two authors contributed equally.Work done during Jian Xie's internship at OSU NLP Group.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Natural Language Processing",
    "sections": [
      "Introduction to the Problem: The Limitations of Static Parametric Memory in Large Language Models",
      "The Role of Tool Augmentation: Tool Augmentation as a Solution to Knowledge Limitations",
      "Research Objectives and Methodology: Investigating LLM Behavior in Knowledge Conflicts: A Controlled Approach",
      "Key Findings: Receptivity to External Evidence: LLMs Can Be Highly Receptive to Coherent External Evidence",
      "Key Findings: Confirmation Bias in Consistent Evidence: LLMs Exhibit Strong Confirmation Bias When Evidence Aligns with Internal Knowledge",
      "Implications for Model Development and Deployment: Implications for the Future of Tool- and Retrieval-Augmented LLMs",
      "Conclusion and Future Directions: Toward More Robust and Adaptive Large Language Models"
    ],
    "keywords": [
      "Large Language Models",
      "Knowledge Conflicts",
      "Tool Augmentation",
      "Retrieval Augmentation",
      "Parametric Memory",
      "External Evidence",
      "Confirmation Bias",
      "Behavioral Analysis",
      "Controlled Experimentation",
      "Counter-Memory Construction",
      "Information Coherence",
      "Evidence Receptivity",
      "Model Adaptability",
      "Cognitive Biases",
      "Memory Conflict Resolution",
      "Systematic Framework",
      "Model Deployment",
      "LLM Development",
      "Internship Research",
      "Knowledge Integration"
    ]
  },
  {
    "_id": "32566",
    "text": "A FrameNet for Danish: This paper presents work on a comprehensive FrameNet for Danish (cf. www.framenet.dk), with over 12.000 frames, and an almost complete coverage of Danish verb lemmas. We discuss design principles and frame roles as well as the distinctional use of valency, syntactic function and semantic noun classes. By converting frame distinctors into Constraint Grammar rules, we were able to build a robust frame tagger for running Danish text, using DanGram parses as input. The combined context-informed coverage of the parser-frametagger was 94.3%, with an overall F-score for frame senses of 85.12.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Language Resources and Annotation",
    "sections": [
      "Introduction to FrameNet for Danish",
      "Design Principles and Methodology",
      "Frame Structure and Role Definitions",
      "Integration with Parsing and Tagging Systems",
      "Evaluation and Performance Metrics",
      "Conclusion and Future Directions"
    ],
    "keywords": [
      "FrameNet",
      "Danish language",
      "Frame semantics",
      "Constraint Grammar",
      "Frame tagging",
      "Valency",
      "Syntactic function",
      "Semantic noun classes",
      "Frame roles",
      "Verb lemmas",
      "DanGram",
      "Parsing",
      "F-score",
      "Frame senses",
      "Context-informed coverage",
      "Frame distinctors",
      "Natural Language Processing",
      "Language resources",
      "Lexical semantics",
      "Robustness",
      "NLP systems",
      "Computational linguistics",
      "Linguistic theory",
      "Language technology",
      "Resource development",
      "Technical elements",
      "Thematic areas"
    ]
  },
  {
    "_id": "35194",
    "text": "A Re-examination of Query Expansion Using Lexical Resources: Query expansion is an effective technique to improve the performance of information retrieval systems. Although hand-crafted lexical resources, such as WordNet, could provide more reliable related terms, previous studies showed that query expansion using only WordNet leads to very limited performance improvement. One of the main challenges is how to assign appropriate weights to expanded terms. In this paper, we re-examine this problem using recently proposed axiomatic approaches and find that, with appropriate term weighting strategy, we are able to exploit the information from lexical resources to significantly improve the retrieval performance.Our empirical results on six TREC collections show that query expansion using only hand-crafted lexical resources leads to significant performance improvement. The performance can be further improved if the proposed method is combined with query expansion using co-occurrence-based resources.",
    "coarse_grained_category": "Computer Science",
    "fine_grained_category": "Information Retrieval",
    "sections": [
      "Introduction to Query Expansion and Its Role in Information Retrieval",
      "Challenges in Using Hand-Crafted Lexical Resources",
      "Axiomatic Approaches for Term Weighting",
      "Proposed Methodology: Integrating Lexical Resources with Axiomatic Weighting",
      "Empirical Evaluation on TREC Collections",
      "Comparative Analysis and Combined Approaches"
    ],
    "keywords": [
      "query expansion",
      "information retrieval systems",
      "lexical resources",
      "wordnet",
      "term weighting strategy",
      "axiomatic approaches",
      "performance improvement",
      "trec collections",
      "co-occurrence based resources",
      "hand crafted lexical resources",
      "retrieval performance",
      "related terms",
      "information retrieval evaluation",
      "lexical semantics",
      "query optimization",
      "natural language processing",
      "text mining",
      "relevance feedback",
      "semantic similarity",
      "resource integration in ir"
    ]
  },
  {
    "_id": "32162",
    "text": "Efficient Multilingual Text Classification for Indian Languages: India is one of the richest language hubs on the earth and is very diverse and multilingual. But apart from a few Indian languages, most of them are still considered to be resource poor. Since most of the NLP techniques either require linguistic knowledge that can only be developed by experts and native speakers of that language or they require a lot of labelled data which is again expensive to generate, the task of text classification becomes challenging for most of the Indian languages. The main objective of this paper is to see how one can benefit from the lexical similarity found in Indian languages in a multilingual scenario. Can a classification model trained on one Indian language be reused for other Indian languages? So, we performed zero-shot text classification via exploiting lexical similarity and we observed that our model performs best in those cases where the vocabulary overlap between the language datasets is maximum. Our experiments also confirm that a single multilingual model trained via exploiting language relatedness outperforms the baselines by significant margins.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction: The Multilingual Challenge in Indian Languages",
      "Challenges in Text Classification for Resource-Poor Languages",
      "Leveraging Lexical Similarity in Multilingual Settings",
      "Methodology: Zero-Shot Text Classification via Lexical Similarity",
      "Experimental Setup and Results",
      "Discussion and Implications"
    ],
    "keywords": [
      "Multilingual Text Classification",
      "Indian Languages",
      "Lexical Similarity",
      "Resource-Poor Languages",
      "Natural Language Processing",
      "Zero-Shot Learning",
      "Text Classification",
      "Language Diversity",
      "Linguistic Knowledge",
      "Labelled Data",
      "Model Reusability",
      "Vocabulary Overlap",
      "Multilingual Models",
      "Language Relatedness",
      "Expert Knowledge",
      "Native Speakers",
      "Data Generation Costs",
      "Baseline Models",
      "Performance Evaluation",
      "Cross-Lingual Transfer"
    ]
  },
  {
    "_id": "18220",
    "text": "Harvesting and Refining Question-Answer Pairs for Unsupervised QA: Question Answering (QA) has shown great success thanks to the availability of largescale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as REFQA). Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RE-FQA. We conduct experiments 1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Question Answering (QA) Systems",
    "sections": [
      "Introduction to Unsupervised Question Answering",
      "Harvesting Diverse Question-Answer Pairs",
      "Refinement of Question-Answer Pairs Using QA Models",
      "Experimental Setup and Evaluation Metrics",
      "Results and Comparative Analysis",
      "Few-Shot Learning Evaluation and Broader Implications"
    ],
    "keywords": [
      "Question Answering",
      "Unsupervised QA",
      "Large-scale Datasets",
      "Neural Models",
      "Few-shot Learning",
      "No-labeled Data",
      "Lexical Diversity",
      "Syntactic Diversity",
      "Wikipedia",
      "REFQA Corpus",
      "Question-Answer Pair",
      "Iterative Refinement",
      "BERT Model",
      "Fine-tuning",
      "SQuAD 1.1",
      "NewsQA",
      "Supervised Learning",
      "Unsupervised Learning",
      "Data Augmentation",
      "Automated Corpus",
      "Model Evaluation"
    ]
  },
  {
    "_id": "24519",
    "text": "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages: Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.",
    "coarse_grained_category": "Human-Computer Interaction (HCI)",
    "fine_grained_category": "Natural Language Processing (NLP) and Text Prediction",
    "sections": [
      "The Role of Text Prediction in Modern Communication",
      "Understanding Text Prediction Algorithms and Their Limitations",
      "The Importance of Contextual Signals in Text Prediction",
      "Contextual Signals in Chat Communication: A Case Study on Microsoft Teams",
      "Contextual Signals in Email Communication: Insights from Outlook",
      "Comparative Analysis: Chat vs. Email Text Prediction with Context"
    ],
    "keywords": [
      "Text Prediction",
      "Contextual Signals",
      "Chat Communication",
      "Email Communication",
      "Productivity",
      "Real-Time Phrase Completion",
      "Large Language Models",
      "Prior Words",
      "Commercial Text Prediction Model",
      "Microsoft Teams",
      "Outlook",
      "Time Context",
      "Previous Messages",
      "Service-Oriented Text Prediction",
      "Performance Metrics",
      "Contextual Awareness",
      "User Interaction",
      "Task Completion",
      "Natural Language Processing",
      "Contextual Adaptation"
    ]
  },
  {
    "_id": "54352",
    "text": "A Simple Method for Tagset Comparison: Based on the idea that local contexts predict the same basic category across a language, we develop a simple method for comparing tagsets across corpora. The principle differences between tagsets are evidenced by variation in categories in one corpus in the same contexts where another corpus exhibits only a single tag. Such mismatches highlight differences in the definitions of tags which are crucial when porting technology from one annotation scheme to another.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP) and Annotation Schemes",
    "sections": [
      "Introduction to Tagset Comparison",
      "Local Contexts Predict Categories",
      "Identifying Tagset Mismatches",
      "Methodology for Tagset Comparison",
      "Applications and Implications",
      "Conclusion and Future Directions"
    ],
    "keywords": [
      "tagset comparison",
      "corpus analysis",
      "annotation schemes",
      "local contexts",
      "tag definitions",
      "language variation",
      "tag mismatches",
      "category variation",
      "cross-corpus analysis",
      "tag portability",
      "linguistic categories",
      "annotation transfer",
      "tagset alignment",
      "corpus-specific tags",
      "semantic equivalence",
      "tagset mapping",
      "lexical contexts",
      "tagset design",
      "computational linguistics",
      "natural language processing"
    ]
  },
  {
    "_id": "50359",
    "text": "GLTR: Statistical Detection and Visualization of Generated Text: The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing and Text Generation",
    "sections": [
      "The Rise of Text Generation and Its Implications",
      "The Need for Simple and Explainable Detection Methods",
      "Overview of GLTR: A Tool for Detecting Generated Text",
      "Methodology: Statistical Techniques for Detecting Generation Artifacts",
      "Evaluation: Human-Subjects Study Results",
      "Implementation and Deployment: Making GLTR Accessible",
      "Applications and Impact: Real-World Use Cases",
      "Conclusion: Advancing Transparency and Trust in AI Systems"
    ],
    "keywords": [
      "Generated Text",
      "Language Models",
      "Text Generation Systems",
      "Abuse of AI",
      "Statistical Detection",
      "GLTR",
      "Human Detection",
      "Fake Text Identification",
      "Open-Source Software",
      "Public Deployment",
      "Sampling Schemes",
      "Annotation Scheme",
      "Human-Subjects Study",
      "Non-Expert Users",
      "Model Outputs",
      "Detection Rate Improvement",
      "Artifacts in Text",
      "AI Accountability",
      "Ethical AI Use",
      "AI Transparency"
    ]
  },
  {
    "_id": "171",
    "text": "Think Visually: Question Answering through Virtual Imagery: In this paper, we study the problem of geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture designed for answering questions that admit latent visual representations. DSMN learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks 1 .",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Computer Vision and Natural Language Processing",
    "sections": [
      "Introduction to Geometric Reasoning in Question Answering",
      "Overview of Dynamic Spatial Memory Network (DSMN)",
      "Design and Functionality of DSMN",
      "Synthetic Benchmarks for Geometric Reasoning: FloorPlanQA and ShapeIntersection",
      "Experimental Setup and Evaluation Metrics",
      "Results and Analysis of DSMN Performance",
      "Discussion of Implications and Future Directions"
    ],
    "keywords": [
      "Geometric Reasoning",
      "Question Answering",
      "Virtual Imagery",
      "Dynamic Spatial Memory Network",
      "Visual Representation",
      "Deep Learning Architecture",
      "Synthetic Benchmarks",
      "FloorPlanQA",
      "ShapeIntersection",
      "Visual Thinking Tasks",
      "Latent Visual Representations",
      "Reasoning Over Visual Data",
      "Spatial Reasoning",
      "Model Evaluation",
      "Experimental Validation",
      "Artificial Intelligence",
      "Computer Vision",
      "Cognitive Modeling",
      "Deep Neural Networks",
      "Visual Question Answering"
    ]
  },
  {
    "_id": "31226",
    "text": "Annotating the Little Prince with Chinese AMRs: Meaning Representation (AMR) is an annotation framework in which the meaning of a full sentence is represented as a rooted, acyclic, directed graph. In this paper, we describe a pilot project in which we develop specifications for the annotation of a Chinese AMR corpus: the Chinese translation of the Little Prince. The interagreement smatch score between the two annotators is 0.83. We also propose to integrate alignment into Chinese AMR annotation.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Semantic Annotation and Representation",
    "sections": [
      "Introduction to Meaning Representation (AMR)",
      "Project Overview: Annotating the Chinese Translation of *The Little Prince*",
      "Methodology for Chinese AMR Annotation",
      "Interannotator Agreement and Evaluation",
      "Integration of Alignment in Chinese AMR Annotation",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Meaning Representation",
      "Annotation Framework",
      "Chinese AMR Corpus",
      "Little Prince",
      "Inter-agreement",
      "SMATCH Score",
      "Annotation Specifications",
      "Natural Language Processing",
      "Semantic Annotation",
      "Graph-based Representation",
      "Rooted Directed Graph",
      "Acyclic Graph",
      "Chinese Translation",
      "Corpus Development",
      "Annotation Pilot Project",
      "Semantic Alignment",
      "Cross-lingual Annotation",
      "Linguistic Annotation",
      "Semantic Parsing",
      "Computational Semantics"
    ]
  },
  {
    "_id": "47162",
    "text": "LIMSI Submission for the WMT'13 Quality Estimation Task: an Experiment with n-gram Posteriors: This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task. Our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation",
    "sections": [
      "Introduction and Motivation",
      "Related Work",
      "Methodology and Features",
      "Data and Preprocessing",
      "Experimental Setup",
      "Results and Discussion"
    ],
    "keywords": [
      "Quality Estimation",
      "Machine Learning",
      "n-gram Posteriors",
      "Shared Task",
      "System Output",
      "Probability Quantification",
      "Language Modeling",
      "Feature Engineering",
      "Machine Translation",
      "Statistical NLP",
      "Language Processing",
      "Evaluation Metrics",
      "Model Performance",
      "Text Generation",
      "Natural Language Understanding",
      "Predictive Modeling",
      "Language Resources",
      "Task-Specific Features",
      "Probabilistic Models",
      "Translation Quality Assessment"
    ]
  },
  {
    "_id": "27656",
    "text": "GRaSP: Grounded Representation and Source Perspective: When people or organizations provide information, they make choices regarding what they include and how they represent it. These two aspects combined (the content and the stance) represent a perspective. Investigating perspectives can provide useful insights into the reliability of information, changes in viewpoints over time, shared beliefs among social or political groups and contrasts with other groups, etc. This paper introduces GRaSP, a generic framework for modeling perspectives and their sources.",
    "coarse_grained_category": "Artificial Intelligence and Computational Linguistics",
    "fine_grained_category": "Perspective Analysis and Information Retrieval",
    "sections": [
      "Introduction to GRaSP: Purpose and Motivation",
      "Understanding Perspectives: Content and Stance",
      "Theoretical Foundations of GRaSP",
      "Modeling Perspectives and Their Sources",
      "Applications and Use Cases of GRaSP",
      "Implications and Future Directions"
    ],
    "keywords": [
      "GRaSP",
      "Grounded Representation",
      "Source Perspective",
      "Information Reliability",
      "Viewpoint Evolution",
      "Shared Beliefs",
      "Group Contrast",
      "Perspective Modeling",
      "Information Sources",
      "Content Selection",
      "Stance Representation",
      "Social Groups",
      "Political Groups",
      "Information Framing",
      "Cognitive Bias",
      "Narrative Construction",
      "Epistemic Trust",
      "Discourse Analysis",
      "Semantic Representation",
      "Contextual Awareness"
    ]
  }
]