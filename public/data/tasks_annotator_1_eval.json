[
  {
    "_id": "7438",
    "text": "Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network: Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model's performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion. 1",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Knowledge Graph Completion",
    "sections": [
      "Introduction to Knowledge Graph Completion Challenges",
      "Dataset Curation for Realistic Knowledge Graph Completion",
      "Proposed Methodology: Deep Convolutional Network for KG Completion",
      "Performance Evaluation and Robustness Analysis",
      "Knowledge Distillation and Student Re-Ranking Network",
      "Entity Re-Ranking for Enhanced KG Completion",
      "Discussion of Results and Implications for Future Work"
    ],
    "keywords": [
      "Knowledge Graph",
      "KG Completion",
      "Stacked Convolutions",
      "Student Re-Ranking Network",
      "Biomedical Knowledge",
      "Encyclopedic Knowledge",
      "Commonsense Knowledge",
      "Deep Convolutional Networks",
      "Textual Entity Representations",
      "Sparse Connectivity",
      "Robustness to Sparsity",
      "Entity Re-Ranking",
      "Model Distillation",
      "Benchmark Datasets",
      "Realistic KG Settings",
      "Candidate Entity Ranking",
      "Deep Learning Models",
      "Knowledge Representation",
      "Graph Neural Networks",
      "Information Retrieval",
      "Technical Components",
      "Conceptual Themes",
      "Architectural Approaches",
      "Methodological Approaches"
    ]
  },
  {
    "_id": "14059",
    "text": "HINT3: Raising the bar for Intent Detection in the Wild: Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domainspecific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Intent Detection in Real-World Applications",
    "sections": [
      "Introduction to Intent Detection Challenges in Real-World Scenarios",
      "Limitations of Existing Intent Detection Datasets",
      "Introducing the New Datasets: Real-World User Queries from Live Chatbots",
      "Methodology: Dataset Creation and Training Setup",
      "Evaluation of NLU Platforms and BERT-Based Classifiers",
      "Analysis of Performance Saturation and Unintended Pattern Learning"
    ],
    "keywords": [
      "intent detection",
      "natural language understanding",
      "chatbot",
      "imbalanced datasets",
      "unintended correlations",
      "domain specific aberrations",
      "benchmarking",
      "real world scenarios",
      "live chatbot data",
      "user queries",
      "training data patterns",
      "bert classifier",
      "nlu platforms",
      "performance saturation",
      "data complexity",
      "perception of intent",
      "unwanted correlations",
      "domain adaptation",
      "machine learning evaluation",
      "intent misclassification"
    ]
  },
  {
    "_id": "28951",
    "text": "Creating the best development corpus for Statistical Machine Translation systems: We propose and study three different novel approaches for tackling the problem of development set selection in Statistical Machine Translation. We focus on a scenario where a machine translation system is leveraged for translating a specific test set, without further data from the domain at hand. Such test set stems from a real application of machine translation, where the texts of a specific e-commerce were to be translated. For developing our development-set selection techniques, we first conducted experiments in a controlled scenario, where labelled data from different domains was available, and evaluated the techniques both with classification and translation quality metrics. Then, the bestperforming techniques were evaluated on the e-commerce data at hand, yielding consistent improvements across two language directions.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation",
    "sections": [
      "Introduction to Development Set Selection in Statistical Machine Translation",
      "Problem Statement and Motivation",
      "Overview of Proposed Approaches",
      "Controlled Experiment Setup and Methodology",
      "Evaluation of Techniques in Controlled Scenario",
      "Application to Real-World E-commerce Data",
      "Results and Performance Analysis on E-commerce Data",
      "Discussion and Implications for Future Work"
    ],
    "keywords": [
      "Statistical Machine Translation",
      "Development Corpus",
      "Development Set Selection",
      "Test Set Translation",
      "E-Commerce Domain",
      "Domain Adaptation",
      "Language Direction",
      "Translation Quality Metrics",
      "Classification Metrics",
      "Controlled Scenario",
      "Labelled Data",
      "Machine Translation System",
      "Real Application",
      "Data Availability Constraints",
      "Performance Evaluation",
      "Cross-Lingual Transfer",
      "Domain-Specific Texts",
      "Translation Systems Optimization",
      "Corpus Development Techniques",
      "Natural Language Processing"
    ]
  },
  {
    "_id": "8978",
    "text": "An Evaluation of Image-based Verb Prediction Models against Human Eye-tracking Data: Recent research in language and vision has developed models for predicting and disambiguating verbs from images. Here, we ask whether the predictions made by such models correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.",
    "coarse_grained_category": "Cognitive Science",
    "fine_grained_category": "Computational Linguistics and Vision",
    "sections": [
      "Contextualizing the Intersection of Language and Vision",
      "Why Study Verb Prediction Models through Human Eye-tracking?",
      "Designing the Experimental Framework",
      "Correlation Between Model Predictions and Human Fixations",
      "Interpreting the Alignment of Model and Human Attention",
      "Implications for Model Design and Human Cognition",
      "Constraints in Model Performance and Human Data Collection",
      "Advancing Visual Verb Understanding Through Human-AI Synergy"
    ],
    "keywords": [
      "image-based verb prediction",
      "human eye-tracking data",
      "verb disambiguation",
      "language and vision integration",
      "visual semantics",
      "salient image regions",
      "human intuitions",
      "visual verbs",
      "eye fixation patterns",
      "verbal-visual alignment",
      "model evaluation",
      "attention mechanisms",
      "semantic grounding",
      "cognitive linguistics",
      "computational modeling",
      "visual attention",
      "task-specific gaze behavior",
      "cross-modal perception",
      "natural language processing",
      "human-computer interaction"
    ]
  },
  {
    "_id": "12505",
    "text": "Nominal Markers and Word Order in Korean: The purpose of this paper is to explore the relationship between case and free word order in Korean by providing proper LP constraints which solve the problems of",
    "coarse_grained_category": "Linguistics",
    "fine_grained_category": "Syntax",
    "sections": [
      "Introduction to Korean Syntax and Free Word Order",
      "Case Marking in Korean: Function and Types",
      "Theoretical Framework: Linearization and Lexical Projection (LP) Constraints",
      "Analysis of Case and Word Order Interaction in Korean Sentences",
      "Proposing LP Constraints to Resolve Word Order Ambiguities",
      "Implications for Korean Syntax and Cross-Linguistic Comparisons"
    ],
    "keywords": [
      "Nominal Markers",
      "Word Order",
      "Korean Language",
      "Case System",
      "Free Word Order",
      "Lexical Phonology",
      "Constraints",
      "Syntactic Structure",
      "Case Assignment",
      "Syntax-Lexicon Interface",
      "Generative Grammar",
      "Phrase Structure Rules",
      "Morphosyntax",
      "Functional Categories",
      "Argument Structure",
      "Typological Variation",
      "Cross-Linguistic Comparison",
      "Pro-drop Languages",
      "Dependency Grammar",
      "Language Typology"
    ]
  },
  {
    "_id": "46036",
    "text": "I do not disagree: Leveraging monolingual alignment to detect disagreement in dialogue: A wide array of natural dialogue discourse can be found on the internet. Previous attempts to automatically determine disagreement between interlocutors in such dialogue have mostly relied on n-gram and grammatical dependency features taken from respondent text. Agreement-disagreement classifiers built upon these baseline features tend to do poorly, yet have proven difficult to improve upon. Using the Internet Argument Corpus, which comprises quote and response post pairs taken from an online debate forum with human-annotated agreement scoring, we introduce semantic environment features derived by comparing quote and response sentences which align well. We show that this method improves classifier accuracy relative to the baseline method namely in the retrieval of disagreeing pairs, which improves from 69% to 77%.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "Dialogue Understanding and Sentiment Analysis",
    "sections": [
      "Introduction to the Problem",
      "Limitations of Existing Approaches",
      "Introducing the Internet Argument Corpus",
      "Proposed Method: Semantic Environment Features",
      "Evaluation of the Proposed Method",
      "Implications and Future Work",
      "Conclusion"
    ],
    "keywords": [
      "disagreement detection",
      "natural dialogue discourse",
      "internet argument corpus",
      "monolingual alignment",
      "agreement-disagreement classification",
      "n-gram features",
      "grammatical dependency features",
      "baseline features",
      "classifier accuracy",
      "semantic environment features",
      "quote-response pairs",
      "online debate forums",
      "human-annotated data",
      "dialogue analysis",
      "text alignment",
      "sentiment analysis",
      "discourse understanding",
      "natural language processing",
      "machine learning models",
      "interlocutor interaction"
    ]
  },
  {
    "_id": "44101",
    "text": "Emotion Analysis Using Latent Affective Folding and Embedding: Though data-driven in nature, emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge in order to isolate the emotional keywords or keysets necessary to the construction of affective categories. This makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse. This paper proposes a more general strategy which leverages two distincts semantic levels, one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction to Emotion Analysis in Text: Challenges in Traditional Approaches and the Need for a More Robust Framework",
      "Limitations of Latent Semantic Analysis in Emotion Detection: The Vulnerability of Expert-Driven Keyword Selection",
      "Conceptual Framework: Dual Semantic Levels: Introducing the Dual Semantic Model for Emotion Analysis",
      "Methodology: Latent Affective Folding and Embedding: A Novel Approach to Capturing Emotional Nuance Through Dual Semantic Encoding",
      "Implementation and Technical Details: Building the Dual Semantic Model: Architecture and Computational Considerations",
      "Empirical Evaluation and Results: Testing the Effectiveness of the Dual Semantic Approach",
      "Discussion of Findings and Implications: Insights from the Data: How Dual Semantics Enhance Emotion Understanding",
      "Conclusion and Future Directions: Advancing Automatic Emotion Detection: Next Steps and Research Opportunities"
    ],
    "keywords": [
      "Emotion Analysis",
      "Latent Affective Folding",
      "Embedding",
      "Latent Semantic Analysis",
      "Expert Knowledge",
      "Emotional Keywords",
      "Affective Categories",
      "Taxonomy of Affective States",
      "Domain of Discourse",
      "Semantic Levels",
      "Domain Foundations",
      "Affective Fabric of Language",
      "Emergent Relationships",
      "Emotion Classification",
      "Automatic Emotion Detection",
      "Text Analysis",
      "Natural Language Processing",
      "Data-Driven Methods",
      "Semantic Modeling",
      "Conceptual Themes"
    ]
  },
  {
    "_id": "19114",
    "text": "Model Selection for Type-Supervised Learning with Application to POS Tagging: Model selection (picking, for example, the feature set and the regularization strength) is crucial for building high-accuracy NLP models. In supervised learning, we can estimate the accuracy of a model on a subset of the labeled data and choose the model with the highest accuracy. In contrast, here we focus on type-supervised learning, which uses constraints over the possible labels for word types for supervision, and labeled data is either not available or very small. For the setting where no labeled data is available, we perform a comparative study of previously proposed and one novel model selection criterion on type-supervised POS-tagging in nine languages. For the setting where a small labeled set is available, we show that the set should be used for semi-supervised learning rather than for model selection onlyusing it for model selection reduces the error by less than 5%, whereas using it for semi-supervised learning reduces the error by 44%.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "Supervised Learning and Model Selection in NLP",
    "sections": [
      "Introduction to Type-Supervised Learning: Understanding the Shift from Traditional Supervised Learning",
      "The Importance of Model Selection in NLP: Why Model Selection Matters for High-Accuracy NLP Models",
      "Challenges in Type-Supervised Learning: Limitations of Labeled Data in Type-Supervised Settings",
      "Model Selection in the Absence of Labeled Data: A Comparative Study of Model Selection Criteria",
      "Leveraging Small Labeled Sets in Type-Supervised Learning: Semi-Supervised Learning Outperforms Model Selection with Limited Labels",
      "Conclusion and Implications for Future Work: Insights and Directions for Research in Type-Supervised Learning"
    ],
    "keywords": [
      "Type-Supervised Learning",
      "Model Selection",
      "POS Tagging",
      "Natural Language Processing",
      "Supervised Learning",
      "Feature Set Selection",
      "Regularization Strength",
      "Accuracy Estimation",
      "Labeled Data",
      "Unlabeled Data",
      "Semi-Supervised Learning",
      "Language Modeling",
      "Cross-Lingual Analysis",
      "Model Evaluation",
      "Error Reduction",
      "Language Specificity",
      "Constraint-Based Supervision",
      "Feature Engineering",
      "Machine Learning Optimization",
      "Comparative Study"
    ]
  },
  {
    "_id": "5146",
    "text": "Analyzing Political Parody in Social Media: Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.",
    "coarse_grained_category": "Computational Linguistics and Natural Language Processing",
    "fine_grained_category": "Political Communication and Social Media Analysis",
    "sections": [
      "Introduction to Political Parody in Social Media",
      "Need for Computational Analysis of Political Parody",
      "Data Collection and Dataset Overview",
      "Methodology for Parody Detection",
      "Results and Model Performance",
      "Linguistic Analysis of Parody Markers"
    ],
    "keywords": [
      "Political Parody",
      "Social Media",
      "Computational Study",
      "Machine Learning",
      "Supervised Models",
      "Parody Detection",
      "Tweet Data",
      "Real Politicians",
      "Parody Accounts",
      "Accuracy",
      "Robustness",
      "Gender Diversity",
      "Cross-Country Analysis",
      "Linguistic Analysis",
      "Fact Checking",
      "Sentiment Analysis",
      "Filtering Out Parodical Utterances",
      "Publicly Available Dataset",
      "Political Communication",
      "Figurative Device"
    ]
  },
  {
    "_id": "34074",
    "text": "GerNED: A German Corpus for Named Entity Disambiguation: Determining the real-world referents for name mentions of persons, organizations and other named entities in texts has become an important task in many information retrieval scenarios and is referred to as Named Entity Disambiguation (NED). While comprehensive datasets support the development and evaluation of NED approaches for English, there are no public datasets to assess NED systems for other languages, such as German. This paper describes the construction of an NED dataset based on a large corpus of German news articles. The dataset is closely modeled on the datasets used for the Knowledge Base Population tasks of the Text Analysis Conference, and contains gold standard annotations for the NED tasks of Entity Linking, NIL Detection and NIL Clustering. We also present first experimental results on the new dataset for each of these tasks in order to establish a baseline for future research efforts.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction to Named Entity Disambiguation (NED)",
      "Motivation and Background",
      "Overview of the GerNED Dataset",
      "Dataset Construction and Annotation Process",
      "Gold Standard Annotations: Entity Linking, NIL Detection, and NIL Clustering",
      "Experimental Results and Baseline Evaluation",
      "Challenges and Limitations",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Named Entity Disambiguation",
      "Entity Linking",
      "NIL Detection",
      "NIL Clustering",
      "Knowledge Base Population",
      "Text Analysis Conference",
      "German Language Processing",
      "Multilingual NLP",
      "Information Retrieval",
      "Corpus Construction",
      "Gold Standard Annotations",
      "Named Entities",
      "Person Mentions",
      "Organization Mentions",
      "Real-World Referents",
      "Baseline Evaluation",
      "Natural Language Processing",
      "News Articles Corpus",
      "Language-Specific Datasets",
      "Cross-Lingual NED Systems"
    ]
  }
]