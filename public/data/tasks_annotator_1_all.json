[
  {
    "_id": "49575",
    "text": "SWEAT: Scoring Polarization of Topics across Different Corpora: Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure.",
    "coarse_grained_category": "Computational Social Sciences",
    "fine_grained_category": "Natural Language Processing and Text Analysis",
    "sections": [
      "Understanding Viewpoint Differences Across Corpora",
      "Measuring Topic Polarization Across Corpora",
      "Introducing SWEAT – Sliced Word Embedding Association Test",
      "Computing Topic Polarization with SWEAT",
      "Assessing the Effectiveness of SWEAT",
      "Applying SWEAT to Real-World Data",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Computational Social Sciences",
      "Topic Polarization",
      "SWEAT",
      "Word Embeddings",
      "Distributional Representations",
      "Statistical Measure",
      "Topical Wordset",
      "Valence",
      "Opposite Valence Wordsets",
      "Corpora",
      "Viewpoint Differences",
      "Polarization Analysis",
      "Case Study",
      "Semantic Similarity",
      "Textual Analysis",
      "Natural Language Processing",
      "Sentiment Analysis",
      "Comparative Linguistics",
      "Computational Methods",
      "Ideological Alignment"
    ]
  },
  {
    "_id": "20130",
    "text": "An Empirical Investigation of Proposals in Collaborative Dialogues: We describe a corpus-based investigation of proposals in dialogue. First, we describe our DR/compliant coding scheme and report our inter-coder reliability results. Next, we test several hypotheses about what constitutes a well-formed proposal.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Dialogue Analysis",
    "sections": [
      "Contextualizing Collaborative Dialogues and the Role of Proposals",
      "Corpus Construction and Coding Scheme Development",
      "Ensuring Consistency in Coding Practices",
      "Formulating Hypotheses About Well-Formed Proposals",
      "Analyzing Proposal Structures in Collaborative Dialogues",
      "Interpreting Results in Context",
      "Summary and Contributions to the Field"
    ],
    "keywords": [
      "Collaborative Dialogues",
      "Proposals",
      "Corpus-Based Investigation",
      "DR/Compliant Coding Scheme",
      "Inter-Coder Reliability",
      "Hypotheses Testing",
      "Well-Formed Proposal",
      "Dialogue Analysis",
      "Empirical Research",
      "Coding Schemes",
      "Communication Patterns",
      "Dialogic Interaction",
      "Language Use in Dialogue",
      "Speech Act Theory",
      "Discourse Analysis",
      "Collaborative Problem Solving",
      "Conversation Structure",
      "Intercoder Agreement",
      "Qualitative Research Methods",
      "Proposal Evaluation Criteria"
    ]
  },
  {
    "_id": "42330",
    "text": "Multilingual Deep Lexical Acquisition for HPSGs via Supertagging: We propose a conditional random fieldbased method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese.Using a pseudo-likelihood approximation we are able to scale our model to hundreds of supertags and tens-of-thousands of training sentences. We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features. Further, we explore the performance of the models at the type-and token-level, demonstrating their superior performance when compared to a unigram-based baseline and a transformation-based learning approach.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP) and Grammar Induction",
    "sections": [
      "Introduction to Multilingual Deep Lexical Acquisition",
      "Methodology: Conditional Random Fields for Supertagging",
      "Application to HPSG-Based Grammars",
      "Scaling with Pseudo-Likelihood Approximation",
      "Experimental Results and Performance Evaluation",
      "Comparative Analysis and Model Insights"
    ],
    "keywords": [
      "HPSG",
      "Supertagging",
      "Conditional Random Fields",
      "Lexical Acquisition",
      "Multilingual NLP",
      "Pseudo-Likelihood Approximation",
      "Language Independence",
      "Lexical Features",
      "Deep Learning",
      "Statistical Parsing",
      "Grammatical Formalism",
      "Natural Language Processing",
      "Model Scaling",
      "Training Sentences",
      "Type-Level Performance",
      "Token-Level Performance",
      "Unigram-Based Baseline",
      "Transformation-Based Learning",
      "Start-of-the-Art Results",
      "Cross-Linguistic Analysis"
    ]
  },
  {
    "_id": "42790",
    "text": "Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR): For a long time, text mining and information extraction for the medical domain has focused on scientific text generated by researchers. However, their direct access to individual patient experiences or patient-doctor interactions is sometimes limited. Information provided on social media, e.g., by patients and their relatives, complements the knowledge available in scientific text. It reflects the patient's journey and their subjective perspective on the process of developing symptoms, being diagnosed and offered a treatment, being cured or learning to live with a medical condition. The value of this type of data is therefore twofold: Firstly, it offers direct access to people's perspectives. Secondly, it might cover information that is not available elsewhere, including self-treatment or self-diagnoses. Named entity recognition and relation extraction are methods to structure information that is available in unstructured text. However, existing medical social media corpora focused on a comparably small set of entities and relations and were focused on particular domains, rather than putting the patient into the center of analyses. With this paper we contribute a corpus with a rich set of annotation layers following the motivation to uncover and model patients' journeys and experiences in more detail. We label 14 entity classes (incl. environmental factors, diagnostics, biochemical processes, patients' quality-of-life descriptions, pathogens, medical conditions, and treatments) and 20 relation classes (e.g., prevents, influences, interactions, causes) most of which have not been considered before for social media data. The publicly available dataset consists of 2,100 tweets with ≈6,000 entity and ≈3,000 relation annotations. In a corpus analysis we find that over 80 % of documents contain relevant entities. Over 50 % of tweets express relations which we consider essential for uncovering patients' narratives about their journeys.",
    "coarse_grained_category": "Health Informatics",
    "fine_grained_category": "Medical Text Mining and Social Media Analysis",
    "sections": [
      "The Limitations of Traditional Medical Text Mining",
      "Social Media as a Source for Patient Narratives",
      "Challenges in Analyzing Unstructured Social Media Content",
      "Building the BEAR Corpus for Patient Journeys",
      "Entity and Relation Classes for Annotation",
      "Analyzing Patient Experiences in the BEAR Corpus",
      "Applications and Future Work of the BEAR Corpus"
    ],
    "keywords": [
      "Patient Journeys",
      "Biomedical Entities",
      "Social Media Data",
      "Twitter Corpus",
      "Named Entity Recognition",
      "Relation Extraction",
      "Medical Text Mining",
      "Information Extraction",
      "Patient Perspectives",
      "Subjective Narratives",
      "Self-Diagnosis",
      "Self-Treatment",
      "Diagnostics",
      "Environmental Factors",
      "Quality of Life",
      "Medical Conditions",
      "Pathogens",
      "Biochemical Processes",
      "Treatment Outcomes",
      "Patient-Doctor Interactions",
      "Health Informatics",
      "Digital Health",
      "Natural Language Processing",
      "Data Annotation",
      "Corpus Linguistics",
      "Clinical Narratives",
      "Healthcare Communication",
      "Patient-Centered Research",
      "Social Media Analytics",
      "Health Behavior Analysis",
      "Medical Ontology",
      "Health Equity",
      "Emotional Expression",
      "Public Health Insights",
      "Narrative Medicine",
      "Health Literacy",
      "Data Privacy",
      "Health Communication",
      "Patient Empowerment",
      "Health Storytelling"
    ]
  },
  {
    "_id": "48482",
    "text": "Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning: Reinforcement learning is widely used for dialogue policy optimization where the reward function often consists of more than one component, e.g., the dialogue success and the dialogue length. In this work, we propose a structured method for finding a good balance between these components by searching for the optimal reward component weighting. To render this search feasible, we use multi-objective reinforcement learning to significantly reduce the number of training dialogues required. We apply our proposed method to find optimized component weights for six domains and compare them to a default baseline.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Natural Language Processing",
    "sections": [
      "Introduction to Reward-Balancing in Spoken Dialogue Systems",
      "Background on Reinforcement Learning in Dialogue Policy Optimization",
      "Multi-Objective Reinforcement Learning for Reward Balancing",
      "Proposed Methodology: Structured Reward Component Weighting Search",
      "Implementation and Application Across Six Domains",
      "Results and Comparison with Baseline Approaches"
    ],
    "keywords": [
      "Statistical Spoken Dialogue Systems",
      "Multi-Objective Reinforcement Learning",
      "Reward Balancing",
      "Dialogue Policy Optimization",
      "Reward Function Components",
      "Dialogue Success",
      "Dialogue Length",
      "Optimal Reward Weighting",
      "Training Dialogue Efficiency",
      "Component Weight Optimization",
      "Reinforcement Learning",
      "Dialogue Domain Adaptation",
      "Baseline Comparison",
      "Policy Search",
      "Reward Function Design",
      "Multi-Objective Optimization",
      "Natural Language Processing",
      "Human-Computer Interaction",
      "Machine Learning in Dialogue Systems",
      "System Performance Evaluation"
    ]
  },
  {
    "_id": "35929",
    "text": "Towards a Model of Prediction-based Syntactic Category Acquisition: First Steps with Word Embeddings: We present a prototype model, based on a combination of count-based distributional semantics and prediction-based neural word embeddings, which learns about syntactic categories as a function of (1) writing contextual, phonological, and lexical-stress-related information to memory and (2) predicting upcoming context words based on memorized information. The system is a first step towards utilizing recently popular methods from Natural Language Processing for exploring the role of prediction in childrens' acquisition of syntactic categories. 1",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Syntax and Language Acquisition",
    "sections": [
      "Role of Prediction in Syntactic Acquisition",
      "Distributional Semantics and Neural Embeddings",
      "Model Architecture with Predictive Mechanisms",
      "Implementation of the Prototype System",
      "Evaluation of Predictive Accuracy and Syntactic Learning",
      "Implications for Child Language Acquisition"
    ],
    "keywords": [
      "Syntactic Category Acquisition",
      "Prediction-based Learning",
      "Word Embeddings",
      "Distributional Semantics",
      "Natural Language Processing",
      "Contextual Information",
      "Phonological Features",
      "Lexical Stress",
      "Memory Systems",
      "Neural Networks",
      "Language Development",
      "Children's Language Acquisition",
      "Predictive Modeling",
      "Language Modeling",
      "Lexical-Phonological Integration",
      "Cognitive Modeling",
      "Computational Linguistics",
      "Modeling Language Structure",
      "Language Processing Mechanisms",
      "Educational Applications"
    ]
  },
  {
    "_id": "41560",
    "text": "Learning Word Representations from Scarce and Noisy Data with Embedding Sub-spaces: We investigate a technique to adapt unsupervised word embeddings to specific applications, when only small and noisy labeled datasets are available. Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "Word Representation Learning",
    "sections": [
      "The Challenge of Adapting Word Embeddings in Scarcely Labeled Tasks",
      "State-of-the-Art Approaches to Word Embedding Adaptation",
      "Embedding Sub-spaces for Task-Specific Adaptation",
      "Technical Framework and Training Procedure",
      "Performance Evaluation on Sentiment Analysis and POS Tagging Tasks",
      "Implications and Directions for Further Research"
    ],
    "keywords": [
      "Word Representations",
      "Unsupervised Learning",
      "Embedding Subspaces",
      "Task-Specific Adaptation",
      "Pre-Trained Embeddings",
      "Overfitting",
      "Scarce Data",
      "Noisy Data",
      "Supervised Learning",
      "Projection Mapping",
      "Semantic Space",
      "Natural Language Processing",
      "Sentiment Analysis",
      "Part-of-Speech Tagging",
      "SemEval Challenge",
      "State-of-the-Art Results",
      "Model Initialization",
      "Data Augmentation",
      "Dimensionality Reduction",
      "Domain Adaptation"
    ]
  },
  {
    "_id": "12437",
    "text": "Authorship Attribution Using Probabilistic Context-Free Grammars: In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Authorship Analysis",
    "sections": [
      "Problem Statement and Motivation for Authorship Attribution",
      "Review of Existing Methods and Theoretical Foundations",
      "Proposed Methodology for PCFG-Based Authorship Modeling",
      "Technical Implementation and Grammar Induction Process",
      "Evaluation Framework and Comparative Experimental Results",
      "Analysis of Findings and Future Research Directions"
    ],
    "keywords": [
      "Authorship Attribution",
      "Probabilistic Context-Free Grammars",
      "Language Modeling",
      "Document Classification",
      "Text Analysis",
      "Machine Learning",
      "Natural Language Processing",
      "Author Identification",
      "Text Mining",
      "Statistical Modeling",
      "Grammatical Structure Analysis",
      "Author Style Recognition",
      "Textual Features Extraction",
      "Model Evaluation",
      "Dataset Analysis",
      "Classification Algorithms",
      "Probabilistic Models",
      "Computational Linguistics",
      "Textual Similarity Measurement",
      "Authorship Verification"
    ]
  },
  {
    "_id": "27764",
    "text": "A Machine Learning Method to Distinguish Machine Translation from Human Translation: This paper introduces a machine learning approach to distinguish machine translation texts from human texts in the sentence level automatically. In stead of traditional methods, we extract some linguistic features only from the target language side to train the prediction model and these features are independent of the source language. Our prediction model presents an indicator to measure how much a sentence generated by a machine translation system looks like a real human translation. Furthermore, the indicator can directly and effectively enhance statistical machine translation systems, which can be proved as BLEU score improvements.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation Evaluation",
    "sections": [
      "Background and Motivation for Machine Translation Detection",
      "Literature Review on Translation Detection Methods",
      "Machine Learning Framework and Model Design",
      "Linguistic Feature Analysis and Selection",
      "Experimental Setup and Performance Evaluation",
      "Integration and Practical Applications of the Detection Method"
    ],
    "keywords": [
      "Machine Learning",
      "Machine Translation",
      "Human Translation",
      "Linguistic Features",
      "Target Language",
      "Source Language",
      "Prediction Model",
      "Statistical Machine Translation",
      "BLEU Score",
      "Sentence-Level Analysis",
      "Automated Detection",
      "Natural Language Processing",
      "Language Modeling",
      "Translation Quality Assessment",
      "Feature Extraction",
      "Language Independence",
      "Translation Systems",
      "Text Classification",
      "Model Enhancement",
      "Translation Similarity Measurement"
    ]
  },
  {
    "_id": "34119",
    "text": "Sharing annotations better: RESTful Open Annotation: Annotations are increasingly created and shared online and connected with web resources such as databases of real-world entities. Recent collaborative efforts to provide interoperability between online annotation tools and resources have introduced the Open Annotation (OA) model, a general framework for representing annotations based on web standards. Building on the OA model, we propose to share annotations over a minimal web interface that conforms to the Representational State Transfer architectural style and uses the JSON for Linking Data representation (JSON-LD). We introduce tools supporting this approach and apply it to several existing annotation clients and servers, demonstrating direct interoperability between tools and resources that were previously unable to exchange information. The specification and tools are available from http://restoa.github.io/.",
    "coarse_grained_category": "Information Technology and Web Standards",
    "fine_grained_category": "Semantic Web and Annotation Technologies",
    "sections": [
      "Introduction to Open Annotation (OA)",
      "The Need for Interoperable Annotation Systems",
      "RESTful Architecture for Annotation Sharing",
      "JSON-LD as a Representation Format",
      "Tools and Implementation Details",
      "Case Studies and Practical Applications",
      "Availability and Further Resources"
    ],
    "keywords": [
      "open annotation",
      "restful api",
      "interoperability",
      "web standards",
      "json-ld",
      "annotation tools",
      "annotation clients",
      "annotation servers",
      "linked data",
      "web resources",
      "real-world entities",
      "collaborative efforts",
      "semantic web",
      "data exchange",
      "minimal web interface",
      "digital annotation",
      "information sharing",
      "web architecture",
      "open standards",
      "annotation interoperability"
    ]
  },
  {
    "_id": "7438",
    "text": "Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network: Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model's performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion. 1",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Knowledge Graph Completion",
    "sections": [
      "Introduction to Knowledge Graph Completion Challenges",
      "Dataset Curation for Realistic Knowledge Graph Completion",
      "Proposed Methodology: Deep Convolutional Network for KG Completion",
      "Performance Evaluation and Robustness Analysis",
      "Knowledge Distillation and Student Re-Ranking Network",
      "Entity Re-Ranking for Enhanced KG Completion",
      "Discussion of Results and Implications for Future Work"
    ],
    "keywords": [
      "Knowledge Graph",
      "KG Completion",
      "Stacked Convolutions",
      "Student Re-Ranking Network",
      "Biomedical Knowledge",
      "Encyclopedic Knowledge",
      "Commonsense Knowledge",
      "Deep Convolutional Networks",
      "Textual Entity Representations",
      "Sparse Connectivity",
      "Robustness to Sparsity",
      "Entity Re-Ranking",
      "Model Distillation",
      "Benchmark Datasets",
      "Realistic KG Settings",
      "Candidate Entity Ranking",
      "Deep Learning Models",
      "Knowledge Representation",
      "Graph Neural Networks",
      "Information Retrieval",
      "Technical Components",
      "Conceptual Themes",
      "Architectural Approaches",
      "Methodological Approaches"
    ]
  },
  {
    "_id": "14059",
    "text": "HINT3: Raising the bar for Intent Detection in the Wild: Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domainspecific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.",
    "coarse_grained_category": "Artificial Intelligence",
    "fine_grained_category": "Intent Detection in Real-World Applications",
    "sections": [
      "Introduction to Intent Detection Challenges in Real-World Scenarios",
      "Limitations of Existing Intent Detection Datasets",
      "Introducing the New Datasets: Real-World User Queries from Live Chatbots",
      "Methodology: Dataset Creation and Training Setup",
      "Evaluation of NLU Platforms and BERT-Based Classifiers",
      "Analysis of Performance Saturation and Unintended Pattern Learning"
    ],
    "keywords": [
      "intent detection",
      "natural language understanding",
      "chatbot",
      "imbalanced datasets",
      "unintended correlations",
      "domain specific aberrations",
      "benchmarking",
      "real world scenarios",
      "live chatbot data",
      "user queries",
      "training data patterns",
      "bert classifier",
      "nlu platforms",
      "performance saturation",
      "data complexity",
      "perception of intent",
      "unwanted correlations",
      "domain adaptation",
      "machine learning evaluation",
      "intent misclassification"
    ]
  },
  {
    "_id": "28951",
    "text": "Creating the best development corpus for Statistical Machine Translation systems: We propose and study three different novel approaches for tackling the problem of development set selection in Statistical Machine Translation. We focus on a scenario where a machine translation system is leveraged for translating a specific test set, without further data from the domain at hand. Such test set stems from a real application of machine translation, where the texts of a specific e-commerce were to be translated. For developing our development-set selection techniques, we first conducted experiments in a controlled scenario, where labelled data from different domains was available, and evaluated the techniques both with classification and translation quality metrics. Then, the bestperforming techniques were evaluated on the e-commerce data at hand, yielding consistent improvements across two language directions.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Machine Translation",
    "sections": [
      "Introduction to Development Set Selection in Statistical Machine Translation",
      "Problem Statement and Motivation",
      "Overview of Proposed Approaches",
      "Controlled Experiment Setup and Methodology",
      "Evaluation of Techniques in Controlled Scenario",
      "Application to Real-World E-commerce Data",
      "Results and Performance Analysis on E-commerce Data",
      "Discussion and Implications for Future Work"
    ],
    "keywords": [
      "Statistical Machine Translation",
      "Development Corpus",
      "Development Set Selection",
      "Test Set Translation",
      "E-Commerce Domain",
      "Domain Adaptation",
      "Language Direction",
      "Translation Quality Metrics",
      "Classification Metrics",
      "Controlled Scenario",
      "Labelled Data",
      "Machine Translation System",
      "Real Application",
      "Data Availability Constraints",
      "Performance Evaluation",
      "Cross-Lingual Transfer",
      "Domain-Specific Texts",
      "Translation Systems Optimization",
      "Corpus Development Techniques",
      "Natural Language Processing"
    ]
  },
  {
    "_id": "8978",
    "text": "An Evaluation of Image-based Verb Prediction Models against Human Eye-tracking Data: Recent research in language and vision has developed models for predicting and disambiguating verbs from images. Here, we ask whether the predictions made by such models correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.",
    "coarse_grained_category": "Cognitive Science",
    "fine_grained_category": "Computational Linguistics and Vision",
    "sections": [
      "Contextualizing the Intersection of Language and Vision",
      "Why Study Verb Prediction Models through Human Eye-tracking?",
      "Designing the Experimental Framework",
      "Correlation Between Model Predictions and Human Fixations",
      "Interpreting the Alignment of Model and Human Attention",
      "Implications for Model Design and Human Cognition",
      "Constraints in Model Performance and Human Data Collection",
      "Advancing Visual Verb Understanding Through Human-AI Synergy"
    ],
    "keywords": [
      "image-based verb prediction",
      "human eye-tracking data",
      "verb disambiguation",
      "language and vision integration",
      "visual semantics",
      "salient image regions",
      "human intuitions",
      "visual verbs",
      "eye fixation patterns",
      "verbal-visual alignment",
      "model evaluation",
      "attention mechanisms",
      "semantic grounding",
      "cognitive linguistics",
      "computational modeling",
      "visual attention",
      "task-specific gaze behavior",
      "cross-modal perception",
      "natural language processing",
      "human-computer interaction"
    ]
  },
  {
    "_id": "12505",
    "text": "Nominal Markers and Word Order in Korean: The purpose of this paper is to explore the relationship between case and free word order in Korean by providing proper LP constraints which solve the problems of",
    "coarse_grained_category": "Linguistics",
    "fine_grained_category": "Syntax",
    "sections": [
      "Introduction to Korean Syntax and Free Word Order",
      "Case Marking in Korean: Function and Types",
      "Theoretical Framework: Linearization and Lexical Projection (LP) Constraints",
      "Analysis of Case and Word Order Interaction in Korean Sentences",
      "Proposing LP Constraints to Resolve Word Order Ambiguities",
      "Implications for Korean Syntax and Cross-Linguistic Comparisons"
    ],
    "keywords": [
      "Nominal Markers",
      "Word Order",
      "Korean Language",
      "Case System",
      "Free Word Order",
      "Lexical Phonology",
      "Constraints",
      "Syntactic Structure",
      "Case Assignment",
      "Syntax-Lexicon Interface",
      "Generative Grammar",
      "Phrase Structure Rules",
      "Morphosyntax",
      "Functional Categories",
      "Argument Structure",
      "Typological Variation",
      "Cross-Linguistic Comparison",
      "Pro-drop Languages",
      "Dependency Grammar",
      "Language Typology"
    ]
  },
  {
    "_id": "46036",
    "text": "I do not disagree: Leveraging monolingual alignment to detect disagreement in dialogue: A wide array of natural dialogue discourse can be found on the internet. Previous attempts to automatically determine disagreement between interlocutors in such dialogue have mostly relied on n-gram and grammatical dependency features taken from respondent text. Agreement-disagreement classifiers built upon these baseline features tend to do poorly, yet have proven difficult to improve upon. Using the Internet Argument Corpus, which comprises quote and response post pairs taken from an online debate forum with human-annotated agreement scoring, we introduce semantic environment features derived by comparing quote and response sentences which align well. We show that this method improves classifier accuracy relative to the baseline method namely in the retrieval of disagreeing pairs, which improves from 69% to 77%.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "Dialogue Understanding and Sentiment Analysis",
    "sections": [
      "Introduction to the Problem",
      "Limitations of Existing Approaches",
      "Introducing the Internet Argument Corpus",
      "Proposed Method: Semantic Environment Features",
      "Evaluation of the Proposed Method",
      "Implications and Future Work",
      "Conclusion"
    ],
    "keywords": [
      "disagreement detection",
      "natural dialogue discourse",
      "internet argument corpus",
      "monolingual alignment",
      "agreement-disagreement classification",
      "n-gram features",
      "grammatical dependency features",
      "baseline features",
      "classifier accuracy",
      "semantic environment features",
      "quote-response pairs",
      "online debate forums",
      "human-annotated data",
      "dialogue analysis",
      "text alignment",
      "sentiment analysis",
      "discourse understanding",
      "natural language processing",
      "machine learning models",
      "interlocutor interaction"
    ]
  },
  {
    "_id": "44101",
    "text": "Emotion Analysis Using Latent Affective Folding and Embedding: Though data-driven in nature, emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge in order to isolate the emotional keywords or keysets necessary to the construction of affective categories. This makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse. This paper proposes a more general strategy which leverages two distincts semantic levels, one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text.",
    "coarse_grained_category": "Artificial Intelligence and Machine Learning",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction to Emotion Analysis in Text: Challenges in Traditional Approaches and the Need for a More Robust Framework",
      "Limitations of Latent Semantic Analysis in Emotion Detection: The Vulnerability of Expert-Driven Keyword Selection",
      "Conceptual Framework: Dual Semantic Levels: Introducing the Dual Semantic Model for Emotion Analysis",
      "Methodology: Latent Affective Folding and Embedding: A Novel Approach to Capturing Emotional Nuance Through Dual Semantic Encoding",
      "Implementation and Technical Details: Building the Dual Semantic Model: Architecture and Computational Considerations",
      "Empirical Evaluation and Results: Testing the Effectiveness of the Dual Semantic Approach",
      "Discussion of Findings and Implications: Insights from the Data: How Dual Semantics Enhance Emotion Understanding",
      "Conclusion and Future Directions: Advancing Automatic Emotion Detection: Next Steps and Research Opportunities"
    ],
    "keywords": [
      "Emotion Analysis",
      "Latent Affective Folding",
      "Embedding",
      "Latent Semantic Analysis",
      "Expert Knowledge",
      "Emotional Keywords",
      "Affective Categories",
      "Taxonomy of Affective States",
      "Domain of Discourse",
      "Semantic Levels",
      "Domain Foundations",
      "Affective Fabric of Language",
      "Emergent Relationships",
      "Emotion Classification",
      "Automatic Emotion Detection",
      "Text Analysis",
      "Natural Language Processing",
      "Data-Driven Methods",
      "Semantic Modeling",
      "Conceptual Themes"
    ]
  },
  {
    "_id": "19114",
    "text": "Model Selection for Type-Supervised Learning with Application to POS Tagging: Model selection (picking, for example, the feature set and the regularization strength) is crucial for building high-accuracy NLP models. In supervised learning, we can estimate the accuracy of a model on a subset of the labeled data and choose the model with the highest accuracy. In contrast, here we focus on type-supervised learning, which uses constraints over the possible labels for word types for supervision, and labeled data is either not available or very small. For the setting where no labeled data is available, we perform a comparative study of previously proposed and one novel model selection criterion on type-supervised POS-tagging in nine languages. For the setting where a small labeled set is available, we show that the set should be used for semi-supervised learning rather than for model selection onlyusing it for model selection reduces the error by less than 5%, whereas using it for semi-supervised learning reduces the error by 44%.",
    "coarse_grained_category": "Natural Language Processing (NLP)",
    "fine_grained_category": "Supervised Learning and Model Selection in NLP",
    "sections": [
      "Introduction to Type-Supervised Learning: Understanding the Shift from Traditional Supervised Learning",
      "The Importance of Model Selection in NLP: Why Model Selection Matters for High-Accuracy NLP Models",
      "Challenges in Type-Supervised Learning: Limitations of Labeled Data in Type-Supervised Settings",
      "Model Selection in the Absence of Labeled Data: A Comparative Study of Model Selection Criteria",
      "Leveraging Small Labeled Sets in Type-Supervised Learning: Semi-Supervised Learning Outperforms Model Selection with Limited Labels",
      "Conclusion and Implications for Future Work: Insights and Directions for Research in Type-Supervised Learning"
    ],
    "keywords": [
      "Type-Supervised Learning",
      "Model Selection",
      "POS Tagging",
      "Natural Language Processing",
      "Supervised Learning",
      "Feature Set Selection",
      "Regularization Strength",
      "Accuracy Estimation",
      "Labeled Data",
      "Unlabeled Data",
      "Semi-Supervised Learning",
      "Language Modeling",
      "Cross-Lingual Analysis",
      "Model Evaluation",
      "Error Reduction",
      "Language Specificity",
      "Constraint-Based Supervision",
      "Feature Engineering",
      "Machine Learning Optimization",
      "Comparative Study"
    ]
  },
  {
    "_id": "5146",
    "text": "Analyzing Political Parody in Social Media: Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.",
    "coarse_grained_category": "Computational Linguistics and Natural Language Processing",
    "fine_grained_category": "Political Communication and Social Media Analysis",
    "sections": [
      "Introduction to Political Parody in Social Media",
      "Need for Computational Analysis of Political Parody",
      "Data Collection and Dataset Overview",
      "Methodology for Parody Detection",
      "Results and Model Performance",
      "Linguistic Analysis of Parody Markers"
    ],
    "keywords": [
      "Political Parody",
      "Social Media",
      "Computational Study",
      "Machine Learning",
      "Supervised Models",
      "Parody Detection",
      "Tweet Data",
      "Real Politicians",
      "Parody Accounts",
      "Accuracy",
      "Robustness",
      "Gender Diversity",
      "Cross-Country Analysis",
      "Linguistic Analysis",
      "Fact Checking",
      "Sentiment Analysis",
      "Filtering Out Parodical Utterances",
      "Publicly Available Dataset",
      "Political Communication",
      "Figurative Device"
    ]
  },
  {
    "_id": "34074",
    "text": "GerNED: A German Corpus for Named Entity Disambiguation: Determining the real-world referents for name mentions of persons, organizations and other named entities in texts has become an important task in many information retrieval scenarios and is referred to as Named Entity Disambiguation (NED). While comprehensive datasets support the development and evaluation of NED approaches for English, there are no public datasets to assess NED systems for other languages, such as German. This paper describes the construction of an NED dataset based on a large corpus of German news articles. The dataset is closely modeled on the datasets used for the Knowledge Base Population tasks of the Text Analysis Conference, and contains gold standard annotations for the NED tasks of Entity Linking, NIL Detection and NIL Clustering. We also present first experimental results on the new dataset for each of these tasks in order to establish a baseline for future research efforts.",
    "coarse_grained_category": "Computational Linguistics",
    "fine_grained_category": "Natural Language Processing (NLP)",
    "sections": [
      "Introduction to Named Entity Disambiguation (NED)",
      "Motivation and Background",
      "Overview of the GerNED Dataset",
      "Dataset Construction and Annotation Process",
      "Gold Standard Annotations: Entity Linking, NIL Detection, and NIL Clustering",
      "Experimental Results and Baseline Evaluation",
      "Challenges and Limitations",
      "Conclusion and Future Work"
    ],
    "keywords": [
      "Named Entity Disambiguation",
      "Entity Linking",
      "NIL Detection",
      "NIL Clustering",
      "Knowledge Base Population",
      "Text Analysis Conference",
      "German Language Processing",
      "Multilingual NLP",
      "Information Retrieval",
      "Corpus Construction",
      "Gold Standard Annotations",
      "Named Entities",
      "Person Mentions",
      "Organization Mentions",
      "Real-World Referents",
      "Baseline Evaluation",
      "Natural Language Processing",
      "News Articles Corpus",
      "Language-Specific Datasets",
      "Cross-Lingual NED Systems"
    ]
  }
]